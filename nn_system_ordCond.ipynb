{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:24:26.063351: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-13 16:24:26.065999: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-13 16:24:26.105709: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-13 16:24:26.105743: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-13 16:24:26.105827: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-13 16:24:26.113671: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-13 16:24:26.114215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 16:24:27.075644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sympy import symbols, sympify, lambdify\n",
    "from tabulate import tabulate\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 2023\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_run.yml', 'r') as ymlfile:\n",
    "    config_data = yaml.load(ymlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_folder': 'ERK_equations_s2p2',\n",
       " 'load_initial_estimate': 'best_estimate_231113_161613',\n",
       " 'save_best_solution': True,\n",
       " 'epochs': 501}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_name = config_data['problem_folder']\n",
    "EQS_FILE = problem_name + '.json'\n",
    "EQS_PATH = os.path.join(os.getcwd(), problem_name, EQS_FILE)\n",
    "\n",
    "\n",
    "training_steps = config_data['epochs']\n",
    "display_step = training_steps // 10\n",
    "\n",
    "load_initial_estimate = config_data['load_initial_estimate']\n",
    "save_best_solution = config_data['save_best_solution']\n",
    "\n",
    "# learning_rate = 1e-2\n",
    "learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay([training_steps // 2],[1e-2, 1e-3])\n",
    "# learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay([training_steps // 3, 2 * training_steps // 3],[1e-2,5e-3,1e-3])\n",
    "# learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay([training_steps // 3, 2 * training_steps // 3],[5e-2,1e-2,5e-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -1.0]\n",
      "[[1.0, 1.0], [2.0]]\n",
      "['b_1', 'b_2', 'a_21*b_2']\n",
      "[b_1, b_2, a_21*b_2]\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON data from the file\n",
    "with open(EQS_PATH, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "variables = data['variables']\n",
    "equations = data['equations']\n",
    "# equation_terms = data['equation_terms']\n",
    "\n",
    "variables_str = ' '.join(variables)\n",
    "variables_sym = symbols(variables_str)\n",
    "equations_sym = [sympify(eq) for eq in equations]  \n",
    "\n",
    "## Identify the equation terms\n",
    "equation_terms = []\n",
    "for (i, eq) in enumerate(equations_sym):\n",
    "    terms = []\n",
    "    for term in eq.args:\n",
    "        terms.append(str(term))\n",
    "    equation_terms.append(terms)\n",
    "\n",
    "# Initialize a list to store the coefficients\n",
    "scalar_coefficients = []\n",
    "functions = []\n",
    "bias_coefficients = []\n",
    "\n",
    "# Loop through the equation terms\n",
    "for terms in equation_terms:\n",
    "    bias_coefficients.append(float(terms[0]))\n",
    "    coefficients = []   \n",
    "    for term in terms[1:]:\n",
    "        # Split each term by the first '*'\n",
    "        term_parts = term.split('*')\n",
    "        # Extract the scalar coefficient or default to '1'\n",
    "        scalar_coeff = term_parts[0] if len(term_parts) > 1 and term_parts[0] not in variables else '1.0'\n",
    "        coefficients.append(float(scalar_coeff))\n",
    "               \n",
    "        # Remove first occurrence of scalar coefficient from the term\n",
    "        func_coeff = term.replace(term_parts[0]+'*', '', 1) if len(term_parts) > 1 and term_parts[0] not in variables else term\n",
    "        functions.append(func_coeff)\n",
    "        \n",
    "    scalar_coefficients.append(coefficients)   \n",
    "       \n",
    "functions_sym = [sympify(func) for func in functions]       \n",
    "        \n",
    "print(bias_coefficients)\n",
    "print(scalar_coefficients)\n",
    "print(functions)\n",
    "print(functions_sym)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_hidden = [3, 3]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'num_output = 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Network Parameters\n",
    "num_input = 1 # input layer\n",
    "\n",
    "num_vars = len(variables)\n",
    "num_eqs = len(equations)\n",
    "num_functions = len(functions)\n",
    "\n",
    "num_hidden = [num_vars, num_functions]\n",
    "num_output = num_eqs # output layer\n",
    "\n",
    "display(\n",
    "    f'num_hidden = {num_hidden}',\n",
    "    f'num_output = {num_output}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial estimate...\n"
     ]
    }
   ],
   "source": [
    "## Weights to Layer 3 - determined by the functions contains the variables\n",
    "w23_flags = [[True] * num_functions for _ in range(num_vars)]\n",
    "for vi, var in enumerate(variables):\n",
    "    for fi, func in enumerate(functions):\n",
    "        # print(f'var = {var}, func = {func}', var not in func)\n",
    "        if var not in func:\n",
    "            w23_flags[vi][fi] = False\n",
    "\n",
    "w23_flags = tf.constant(w23_flags, dtype=tf.bool)\n",
    "\n",
    "# Initialize the weights (w23) with zeros\n",
    "w23 = tf.constant(tf.zeros(num_hidden, dtype=tf.float32))\n",
    "# Set the weights to 1 where func_flags is True\n",
    "w23 = tf.where(w23_flags, tf.ones_like(w23), w23)\n",
    "w23 = tf.transpose(w23)\n",
    "\n",
    "## Weights to Layer 4 - determined by the coefficients of the functions\n",
    "# Initialize the weights (w34) with zeros\n",
    "w34_np = np.zeros([num_functions, num_output], dtype=np.float32)\n",
    "# Assign scalar coefficients to the first column of w34_np\n",
    "row = 0\n",
    "for sci, scalar_coeffs in enumerate(scalar_coefficients):\n",
    "    for scj, scl_coeff in enumerate(scalar_coeffs):\n",
    "        w34_np[row, sci] = scl_coeff\n",
    "        row += 1\n",
    "# Tranform to tensor\n",
    "w34 = tf.constant(w34_np, dtype=tf.float32)\n",
    "\n",
    "## IC for classic methods\n",
    "# w12_rk2 = tf.Variable([[0.8, 0.3, 0.3]])\n",
    "# w12_rk4 = tf.Variable([[0.3, -0.1, 0.63, 0.2, 0.1, 0.8, 0.1, 0.2, 0.4, 0.1]])\n",
    "# w12_r_s5p5 = tf.Variable([[0.35030925,  0.0, 0.43791163, 0.07676958, 0.0, 0.6589101,\n",
    "#                             0.21143106, 0.31450838, 0.0, 0.34649932, 0.17551975, 0.23256625,\n",
    "#                             0.18079841, 0.17328405, 0.23783126]], dtype=tf.float32,  constraint=NonNeg())\n",
    "# w12_vicky = tf.Variable([[0.0, 0.0, 0.0, 1.5, 0.0, 0.0, 0.8, 0.8, 0.0, 0.8, 0.0, 0.9, 0.2, 0.0, 0.0, 1.0]],\n",
    "#                 dtype=tf.float32, \n",
    "#                 constraint=NonNeg())\n",
    "\n",
    "if load_initial_estimate is None:\n",
    "    w12 =  tf.Variable(tf.random.normal([num_input, num_hidden[0]]), \n",
    "                  dtype=tf.float32, \n",
    "                  constraint=NonNeg())\n",
    "    \n",
    "    initial_solution = w12.numpy()[0].tolist()\n",
    "    \n",
    "else:\n",
    "    print('Loading initial estimate...')\n",
    "    # Load json file with initial estimate (best from previous run)\n",
    "    with open(os.path.join(problem_name, f'{load_initial_estimate}.json'), 'r') as file:\n",
    "        data = json.load(file)\n",
    "    init_estimate = data['best_solution']\n",
    "    initial_solution = [float(x) for x in init_estimate.values()]\n",
    "    w12 = tf.Variable([initial_solution], dtype=tf.float32, constraint=NonNeg())\n",
    "    \n",
    "    \n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    ## Variables - weights to Layer 1\n",
    "    # Create the variable with the non-negativity constraint\n",
    "    'w12': w12,\n",
    "    # Whether the functions in F1 and F2 contain the variables x1 and x2\n",
    "    'w23': w23,\n",
    "    # # The coefficients of the functions in F1 and F2\n",
    "    'w34': w34\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b12': tf.constant(tf.zeros([num_hidden[0]], dtype=tf.float32)),\n",
    "    'b23': tf.constant(tf.zeros([num_hidden[1]], dtype=tf.float32)),\n",
    "    'out': tf.constant([bias_coefficients], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = Adam(learning_rate=learning_rate, name='custom_optimizer_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[0.7293615 , 0.31465805, 0.6854762 ]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['w12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_tf_function(inputs, values, symbolic_function):\n",
    "    # Ensure that the number of inputs and values match\n",
    "    if len(inputs) != len(values):\n",
    "        raise ValueError(\"Number of inputs and values must match.\")\n",
    "\n",
    "    # Evaluate the symbolic function using TensorFlow and the provided values\n",
    "    result = symbolic_function(*values)\n",
    "    # Convert the result to a TensorFlow tensor\n",
    "    result = tf.convert_to_tensor(result, dtype=tf.float32)\n",
    "    \n",
    "    # print(result.__class__)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Define the custom activation function with @tf.function\n",
    "# @tf.function\n",
    "def activation_layer2(layer, vars=variables_sym, funcs=functions_sym):\n",
    "    \n",
    "    layer_values = layer.numpy()[0]\n",
    "    # var_vals = [layer_values[i] for i in range(layer_values.shape[0])]\n",
    "    ##x1_val = tf.squeeze(layer[0, 0])\n",
    "    var_vals = [tf.squeeze(layer[0, i]) for i in range(layer_values.shape[0])]\n",
    "    \n",
    "    # print(f'var_vals = {var_vals}')\n",
    "   \n",
    "    # var_vals_dict = dict(zip(vars, var_vals))\n",
    "    # print(f'var_vals_dict = {var_vals_dict}')\n",
    "    # # Step 3 and 4: Substitute values into the functions and evaluate\n",
    "    # layer_act = [func.subs(var_vals_dict).evalf() for func in funcs]\n",
    "    # # Convert SymPy Float to Python float\n",
    "    # layer_act = [tf.convert_to_tensor(float(val.evalf()), dtype=tf.float32) for val in layer_act]\n",
    "    \n",
    "    layer_act = list()\n",
    "    for func in funcs:\n",
    "        func_tf = lambdify(vars, func, 'tensorflow')\n",
    "        layer_act.append(evaluate_tf_function(variables_sym, var_vals, func_tf))\n",
    "        # result = func_tf(*var_vals)\n",
    "        # layer_act.append(result)\n",
    "    \n",
    "    # print(f'layer_act = {layer_act}')\n",
    "    \n",
    "    # layer_2 = tf.stack(layer_act, axis=0)  # Stack the list of tensors into a single tensor\n",
    "    # layer_2 = tf.reshape(layer_2, [1, len(funcs)])\n",
    "    \n",
    "    # Convert layer_act to a TensorFlow tensor\n",
    "    layer_2 = tf.convert_to_tensor(layer_act, dtype=tf.float32)\n",
    "    \n",
    "    # print(type(layer_2))\n",
    "    \n",
    "    # print(f'layer_2 = {layer_2}')\n",
    "    \n",
    "    layer_2 = tf.reshape(layer_2, [1, len(funcs)])\n",
    "\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(weights, biases):\n",
    "\n",
    "    # # Reshape input if necessary, matching the shape of the first layer's weights\n",
    "    # x = tf.reshape(x, [1, -1])  # Adjust the shape as needed\n",
    "\n",
    "    # layer_1 = tf.add(tf.matmul(x, weights['w12']), biases['b12'])\n",
    "    # layer_1 = tf.matmul(x, weights['w12'])\n",
    "    layer_1 = weights['w12']\n",
    "    \n",
    "    # print(f'layer_1 = {layer_1.shape}')\n",
    "\n",
    "    layer_2 = activation_layer2(layer_1)\n",
    "    \n",
    "    # Output fully connected layer\n",
    "    output = tf.add(tf.matmul(layer_2, weights['w34']), biases['out'])\n",
    "    \n",
    "    # # Add RELU activation function to output layer\n",
    "    # output = tf.nn.relu(output)\n",
    "    \n",
    "    return output, layer_1\n",
    "\n",
    "\n",
    "def loss_function(weights, biases):\n",
    "    \n",
    "    output, _ = multilayer_perceptron(weights, biases)\n",
    "\n",
    "    return tf.reduce_mean(tf.square(output))\n",
    "\n",
    "\n",
    "# Train step\n",
    "def train_step(weights, biases, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss = loss_function(weights, biases)\n",
    "\n",
    "    trainable_variables = [weights['w12']]  # list containing only 'w12'\n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      0 => loss: 1.2222274747841766e-08 | best loss: 1.2222274747841766e-08 (0)\n",
      "epoch     50 => loss: 6.3245090586860897e-07 | best loss: 1.2222274747841766e-08 (0)\n",
      "epoch    100 => loss: 4.4951846689400554e-09 | best loss: 3.5818459309666650e-10 (97)\n",
      "epoch    150 => loss: 3.0155433705658652e-11 | best loss: 1.0302869668521453e-12 (144)\n",
      "epoch    200 => loss: 1.7763568394002505e-13 | best loss: 1.4210854715202004e-14 (198)\n",
      "epoch    220 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    250 => loss: 3.5527136788005009e-14 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    258 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    275 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    280 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    289 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    294 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    300 => loss: 1.7763568394002505e-15 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    303 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    308 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    317 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    322 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    330 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    331 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    332 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    333 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    334 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    335 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    336 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    337 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    338 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    339 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    340 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    341 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    342 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    343 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    344 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    345 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    346 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    347 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    348 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    349 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    350 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    350 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    351 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    352 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    353 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    354 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    355 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    356 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    357 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    358 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    359 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    360 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    361 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    362 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    363 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    364 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    365 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    366 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    367 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    368 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    369 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    370 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    371 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    372 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    373 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    374 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    375 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    376 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    377 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    378 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    379 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    380 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    381 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    382 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    383 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    384 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    385 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    386 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    387 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    388 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    389 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    390 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    391 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    392 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    393 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    394 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    395 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    396 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    397 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    398 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    399 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    400 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    400 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    401 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    402 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    403 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    404 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    405 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    406 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    407 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    408 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    409 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    410 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    411 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    412 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    413 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    414 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    415 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    416 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    417 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    418 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    419 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    420 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    421 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    422 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    423 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    424 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    425 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    426 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    427 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    428 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    429 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    430 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    431 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    432 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    433 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    434 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    435 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    436 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    437 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    438 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    439 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    440 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    441 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    442 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    443 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    444 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    445 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    446 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    447 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    448 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    449 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    450 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    450 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    451 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    452 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    453 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    454 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    455 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    456 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    457 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    458 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    459 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    460 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    461 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    462 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    463 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    464 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    465 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    466 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    467 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    468 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    469 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    470 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    471 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    472 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    473 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    474 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    475 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    476 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    477 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    478 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    479 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    480 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    481 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    482 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    483 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    484 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    485 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    486 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    487 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    488 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    489 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    490 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    491 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    492 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    493 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    494 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    495 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    496 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    497 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    498 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    499 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    500 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n",
      "epoch    500 => loss: 0.0000000000000000e+00 | best loss: 0.0000000000000000e+00 (220)\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.inf  # Initialize with infinity or an appropriate initial value\n",
    "\n",
    "# Initialize variables to store the best solution\n",
    "best_solution = None\n",
    "best_epoch = None\n",
    "# # Define the number of epochs after which to save the best weights\n",
    "# save_interval = training_steps // 100    # Every 1% of the training steps \n",
    "\n",
    "for i in range(training_steps):\n",
    "    \n",
    "    current_loss = train_step(weights, biases, optimizer)\n",
    "    \n",
    "    # Check if the current loss is better than the best loss\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        best_epoch = i\n",
    "        best_solution = weights['w12'].numpy()[0].copy()\n",
    "       \n",
    "    if i % display_step == 0:\n",
    "        print(f\"epoch {i:6d} => loss: {current_loss:.16e} | best loss: {best_loss:.16e} ({best_epoch})\")\n",
    "        \n",
    "    if current_loss <= 1e-16:\n",
    "        print(f\"epoch {i:6d} => loss: {current_loss:.16e} | best loss: {best_loss:.16e} ({best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_best_solution is not None:\n",
    "    \n",
    "    best_solution_dict = dict()\n",
    "    initial_solution_dict = dict()\n",
    "    for vi, var in enumerate(variables):\n",
    "        initial_solution_dict[var] = f'{initial_solution[vi]:.16f}'\n",
    "        best_solution_dict[var] = f'{best_solution[vi]:.16f}'\n",
    "        \n",
    "    save_solution_dict = {\n",
    "        'initial_estimate': initial_solution_dict,\n",
    "        'best_solution': best_solution_dict,\n",
    "        'best_epoch': best_epoch,\n",
    "    }\n",
    "        \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%y%m%d %H%M%S\")\n",
    "    save_file_path = os.path.join(problem_name, f'best_estimate_{dt_string.split()[0]}_{dt_string.split()[1]}.json')\n",
    "    \n",
    "    # Save the configuration data to the JSON file\n",
    "    with open(save_file_path, 'w') as config_file:\n",
    "        json.dump(save_solution_dict, config_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7287481, 0.3138917, 0.6861083], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST SOLUTION\n",
      "a_21 = 0.7287480831146240\n",
      "b_1 = 0.3138917088508606\n",
      "b_2 = 0.6861082911491394\n",
      "\n",
      "RESIDUALS\n",
      "+------------------+------------------+---------------------+\n",
      "| Equation         |   Model residual |   Function residual |\n",
      "+==================+==================+=====================+\n",
      "| b_1 + b_2 - 1 =  |                0 |          0          |\n",
      "+------------------+------------------+---------------------+\n",
      "| 2*a_21*b_2 - 1 = |                0 |          2.0397e-07 |\n",
      "+------------------+------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "func_res_model, solution  = multilayer_perceptron(weights, biases)\n",
    "solution = list(solution.numpy()[0])\n",
    "best_solution = list(best_solution)\n",
    "\n",
    "print('BEST SOLUTION')\n",
    "for i, var in enumerate(variables):\n",
    "    print(f'{var} = {best_solution[i]:.16f}')\n",
    "    \n",
    "print()\n",
    "print('RESIDUALS')\n",
    "headers = ['Equation', 'Model residual', 'Function residual']\n",
    "table_data = []\n",
    "\n",
    "for f, func in enumerate(equations):\n",
    "    eq = equations_sym[f]\n",
    "    equation = lambdify(variables_sym, eq)\n",
    "    func_res_eq = abs(equation(*best_solution))\n",
    "    table_data.append([f'{func} = ', f'{abs(func_res_model[0, f]):.4e}', f'{func_res_eq:.4e}'])\n",
    "\n",
    "print(tabulate(table_data, headers=headers, tablefmt='grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nn-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
