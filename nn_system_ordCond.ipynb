{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 11:28:38.015794: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-10 11:28:38.017738: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 11:28:38.063718: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-10 11:28:38.063769: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-10 11:28:38.063808: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-10 11:28:38.070781: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 11:28:38.071551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-10 11:28:38.937107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sympy import symbols, sympify, lambdify\n",
    "from tabulate import tabulate\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 2023\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQS_FILE = 'ERK_equations_s2p2.json'\n",
    "# EQS_FILE = 'ERK_equations_s4p4.json'\n",
    "EQS_FILE = 'vicky_case2.json'\n",
    "\n",
    "EQS_PATH = os.path.join(os.getcwd(), 'RK_rootedtrees', EQS_FILE)\n",
    "\n",
    "\n",
    "training_steps = 50001  #   5000 + 1\n",
    "display_step = training_steps // 10\n",
    "\n",
    "# learning_rate = 1e-2\n",
    "learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay([training_steps // 3, 2 * training_steps // 3],[1e-2,5e-3,1e-3])\n",
    "# learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay([training_steps // 3, 2 * training_steps // 3],[5e-2,1e-2,5e-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equation_terms = [['-1', 'a1', 'a2', 'a3', 'a4'], ['-1', 'b1', 'b2', 'b3', 'b4'], ['-1', 'c1', 'c2', 'c3', 'c4'], ['-1', 'd1', 'd2', 'd3', 'd4'], ['-0.5', 'a2*b1', 'a3*(b1+b2)', 'a4*(b1+b2+b3)'], ['-0.5', 'a2*c1', 'a3*(c1+c2)', 'a4*(c1+c2+c3)'], ['-0.5', 'a2*d1', 'a3*(d1+d2)', 'a4*(d1+d2+d3)'], ['-0.5', 'b2*c1', 'b3*(c1+c2)', 'b4*(c1+c2+c3)'], ['-0.5', 'b2*d1', 'b3*(d1+d2)', 'b4*(d1+d2+d3)'], ['-0.5', 'c2*d1', 'c3*(d1+d2)', 'c4*(d1+d2+d3)']]\n",
      "[-1.0, -1.0, -1.0, -1.0, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5]\n",
      "[[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n",
      "['a1', 'a2', 'a3', 'a4', 'b1', 'b2', 'b3', 'b4', 'c1', 'c2', 'c3', 'c4', 'd1', 'd2', 'd3', 'd4', 'a2*b1', 'a3*(b1+b2)', 'a4*(b1+b2+b3)', 'a2*c1', 'a3*(c1+c2)', 'a4*(c1+c2+c3)', 'a2*d1', 'a3*(d1+d2)', 'a4*(d1+d2+d3)', 'b2*c1', 'b3*(c1+c2)', 'b4*(c1+c2+c3)', 'b2*d1', 'b3*(d1+d2)', 'b4*(d1+d2+d3)', 'c2*d1', 'c3*(d1+d2)', 'c4*(d1+d2+d3)']\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON data from the file\n",
    "with open(EQS_PATH, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "variables = data['variables']\n",
    "equations = data['equations']\n",
    "equation_terms = data['equation_terms']\n",
    "\n",
    "# display(\n",
    "#     f'variables = {variables}',\n",
    "#     f'equations = {equations}',\n",
    "#     f'equation_terms = {equation_terms}'\n",
    "# )\n",
    "\n",
    "print(f'equation_terms = {equation_terms}')\n",
    "\n",
    "# Initialize a list to store the coefficients\n",
    "scalar_coefficients = []\n",
    "functions = []\n",
    "bias_coefficients = []\n",
    "\n",
    "# Loop through the equation terms\n",
    "for terms in equation_terms:\n",
    "    bias_coefficients.append(float(terms[0]))\n",
    "    coefficients = []   \n",
    "    for term in terms[1:]:\n",
    "        # Split each term by the first '*'\n",
    "        term_parts = term.split('*')\n",
    "        # Extract the scalar coefficient or default to '1'\n",
    "        scalar_coeff = term_parts[0] if len(term_parts) > 1 and term_parts[0] not in variables else '1.0'\n",
    "        coefficients.append(float(scalar_coeff))\n",
    "               \n",
    "        # Remove first occurrence of scalar coefficient from the term\n",
    "        func_coeff = term.replace(term_parts[0]+'*', '', 1) if len(term_parts) > 1 and term_parts[0] not in variables else term\n",
    "        functions.append(func_coeff)\n",
    "        \n",
    "    scalar_coefficients.append(coefficients)   \n",
    "        \n",
    "print(bias_coefficients)\n",
    "print(scalar_coefficients)\n",
    "print(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_hidden = [16, 34]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'num_output = 10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Network Parameters\n",
    "num_input = 1 # input layer\n",
    "\n",
    "num_vars = len(variables)\n",
    "num_eqs = len(equations)\n",
    "num_functions = len(functions)\n",
    "\n",
    "num_hidden = [num_vars, num_functions]\n",
    "num_output = num_eqs # output layer\n",
    "\n",
    "display(\n",
    "    f'num_hidden = {num_hidden}',\n",
    "    f'num_output = {num_output}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights to Layer 3 - determined by the functions contains the variables\n",
    "w23_flags = [[True] * num_functions for _ in range(num_vars)]\n",
    "for vi, var in enumerate(variables):\n",
    "    for fi, func in enumerate(functions):\n",
    "        # print(f'var = {var}, func = {func}', var not in func)\n",
    "        if var not in func:\n",
    "            w23_flags[vi][fi] = False\n",
    "\n",
    "w23_flags = tf.constant(w23_flags, dtype=tf.bool)\n",
    "\n",
    "# Initialize the weights (w23) with zeros\n",
    "w23 = tf.constant(tf.zeros(num_hidden, dtype=tf.float32))\n",
    "# Set the weights to 1 where func_flags is True\n",
    "w23 = tf.where(w23_flags, tf.ones_like(w23), w23)\n",
    "w23 = tf.transpose(w23)\n",
    "\n",
    "## Weights to Layer 4 - determined by the coefficients of the functions\n",
    "# Initialize the weights (w34) with zeros\n",
    "w34_np = np.zeros([num_functions, num_output], dtype=np.float32)\n",
    "# Assign scalar coefficients to the first column of w34_np\n",
    "row = 0\n",
    "for sci, scalar_coeffs in enumerate(scalar_coefficients):\n",
    "    for scj, scl_coeff in enumerate(scalar_coeffs):\n",
    "        w34_np[row, sci] = scl_coeff\n",
    "        row += 1\n",
    "# Tranform to tensor\n",
    "w34 = tf.constant(w34_np, dtype=tf.float32)\n",
    "\n",
    "## IC for classic methods\n",
    "# w12_rk2 = tf.Variable([[0.8, 0.3, 0.3]])\n",
    "# w12_rk4 = tf.Variable([[0.3, -0.1, 0.63, 0.2, 0.1, 0.8, 0.1, 0.2, 0.4, 0.1]])\n",
    "# w12_vicky = tf.Variable([[0.0, 0.0, 0.0, 1.5, 0.0, 0.0, 0.8, 0.8, 0.0, 0.8, 0.0, 0.9, 0.2, 0.0, 0.0, 1.0]],\n",
    "#                 dtype=tf.float32, \n",
    "#                 constraint=NonNeg())\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    ## Variables - weights to Layer 1\n",
    "    # Create the variable with the non-negativity constraint\n",
    "    'w12': tf.Variable(tf.random.normal([num_input, num_hidden[0]]), \n",
    "                  dtype=tf.float32, \n",
    "                  constraint=NonNeg()),\n",
    "    # 'w12': w12_rk2,\n",
    "    # 'w12': w12_rk4,\n",
    "    # 'w12': w12_vicky,\n",
    "    # Whether the functions in F1 and F2 contain the variables x1 and x2\n",
    "    'w23': w23,\n",
    "    # # The coefficients of the functions in F1 and F2\n",
    "    'w34': w34\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b12': tf.constant(tf.zeros([num_hidden[0]], dtype=tf.float32)),\n",
    "    'b23': tf.constant(tf.zeros([num_hidden[1]], dtype=tf.float32)),\n",
    "    'out': tf.constant([bias_coefficients], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = Adam(learning_rate=learning_rate, name='custom_optimizer_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_str = ' '.join(variables)\n",
    "variables_sym = symbols(variables_str)\n",
    "functions_sym = [sympify(func) for func in functions]   \n",
    "equations_sym = [sympify(eq) for eq in equations]  \n",
    "\n",
    "def evaluate_tf_function(inputs, values, symbolic_function):\n",
    "    # Ensure that the number of inputs and values match\n",
    "    if len(inputs) != len(values):\n",
    "        raise ValueError(\"Number of inputs and values must match.\")\n",
    "\n",
    "    # Evaluate the symbolic function using TensorFlow and the provided values\n",
    "    result = symbolic_function(*values)\n",
    "    # Convert the result to a TensorFlow tensor\n",
    "    result = tf.convert_to_tensor(result, dtype=tf.float32)\n",
    "    \n",
    "    # print(result.__class__)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Define the custom activation function with @tf.function\n",
    "# @tf.function\n",
    "def activation_layer2(layer, vars=variables_sym, funcs=functions_sym):\n",
    "    \n",
    "    layer_values = layer.numpy()[0]\n",
    "    # var_vals = [layer_values[i] for i in range(layer_values.shape[0])]\n",
    "    ##x1_val = tf.squeeze(layer[0, 0])\n",
    "    var_vals = [tf.squeeze(layer[0, i]) for i in range(layer_values.shape[0])]\n",
    "    \n",
    "    # print(f'var_vals = {var_vals}')\n",
    "   \n",
    "    # var_vals_dict = dict(zip(vars, var_vals))\n",
    "    # print(f'var_vals_dict = {var_vals_dict}')\n",
    "    # # Step 3 and 4: Substitute values into the functions and evaluate\n",
    "    # layer_act = [func.subs(var_vals_dict).evalf() for func in funcs]\n",
    "    # # Convert SymPy Float to Python float\n",
    "    # layer_act = [tf.convert_to_tensor(float(val.evalf()), dtype=tf.float32) for val in layer_act]\n",
    "    \n",
    "    layer_act = list()\n",
    "    for func in funcs:\n",
    "        func_tf = lambdify(vars, func, 'tensorflow')\n",
    "        layer_act.append(evaluate_tf_function(variables_sym, var_vals, func_tf))\n",
    "        # result = func_tf(*var_vals)\n",
    "        # layer_act.append(result)\n",
    "    \n",
    "    # print(f'layer_act = {layer_act}')\n",
    "    \n",
    "    # layer_2 = tf.stack(layer_act, axis=0)  # Stack the list of tensors into a single tensor\n",
    "    # layer_2 = tf.reshape(layer_2, [1, len(funcs)])\n",
    "    \n",
    "    # Convert layer_act to a TensorFlow tensor\n",
    "    layer_2 = tf.convert_to_tensor(layer_act, dtype=tf.float32)\n",
    "    \n",
    "    # print(type(layer_2))\n",
    "    \n",
    "    # print(f'layer_2 = {layer_2}')\n",
    "    \n",
    "    layer_2 = tf.reshape(layer_2, [1, len(funcs)])\n",
    "\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(weights, biases):\n",
    "\n",
    "    # # Reshape input if necessary, matching the shape of the first layer's weights\n",
    "    # x = tf.reshape(x, [1, -1])  # Adjust the shape as needed\n",
    "\n",
    "    # layer_1 = tf.add(tf.matmul(x, weights['w12']), biases['b12'])\n",
    "    # layer_1 = tf.matmul(x, weights['w12'])\n",
    "    layer_1 = weights['w12']\n",
    "    \n",
    "    # print(f'layer_1 = {layer_1.shape}')\n",
    "\n",
    "    layer_2 = activation_layer2(layer_1)\n",
    "    \n",
    "    # Output fully connected layer\n",
    "    output = tf.add(tf.matmul(layer_2, weights['w34']), biases['out'])\n",
    "    \n",
    "    # # Add RELU activation function to output layer\n",
    "    # output = tf.nn.relu(output)\n",
    "    \n",
    "    return output, layer_1\n",
    "\n",
    "\n",
    "def loss_function(weights, biases):\n",
    "    \n",
    "    output, _ = multilayer_perceptron(weights, biases)\n",
    "\n",
    "    return tf.reduce_mean(tf.square(output))\n",
    "\n",
    "\n",
    "# Train step\n",
    "def train_step(weights, biases, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss = loss_function(weights, biases)\n",
    "\n",
    "    trainable_variables = [weights['w12']]  # list containing only 'w12'\n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      0 => loss: 9.3684492111206055e+00 \n",
      "epoch   3000 => loss: 1.3203848538978491e-06 \n",
      "epoch   6000 => loss: 1.3485791328093910e-07 \n",
      "epoch   9000 => loss: 2.4893875405496146e-08 \n",
      "epoch  12000 => loss: 4.2713468317856496e-09 \n"
     ]
    }
   ],
   "source": [
    "for i in range(training_steps):\n",
    "       \n",
    "    current_loss = train_step(weights, biases, optimizer)\n",
    "    if i % display_step == 0:\n",
    "        print(f\"epoch {i:6d} => loss: {current_loss:.16e} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLUTION\n",
      "a1 = 0.0572759769856930\n",
      "a2 = -0.0000000000000000\n",
      "a3 = 0.4697333574295044\n",
      "a4 = 0.4729906320571899\n",
      "b1 = 0.0003403812006582\n",
      "b2 = 0.5300319790840149\n",
      "b3 = -0.0000000000000000\n",
      "b4 = 0.4696328043937683\n",
      "c1 = 0.0572788193821907\n",
      "c2 = 0.0002148825733457\n",
      "c3 = 0.9425117969512939\n",
      "c4 = -0.0000000000000000\n",
      "d1 = 0.4733958840370178\n",
      "d2 = 0.0569879002869129\n",
      "d3 = -0.0000000000000000\n",
      "d4 = 0.4696166217327118\n",
      "\n",
      "RESIDUALS\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| Equation                                   |   Model residual |   Function residual |\n",
      "+============================================+==================+=====================+\n",
      "| a1 + a2 + a3 + a4 - 1 =                    |       5.9605e-08 |          5.9605e-08 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| b1 + b2 + b3 + b4 - 1 =                    |       5.2452e-06 |          5.2452e-06 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| c1 + c2 + c3 + c4 - 1 =                    |       5.4836e-06 |          5.4836e-06 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| d1 + d2 + d3 + d4 - 1 =                    |       3.5763e-07 |          3.5763e-07 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| a2*b1 + a3*(b1+b2) + a4*(b1+b2+b3) - 0.5 = |       5.2452e-06 |          5.2452e-06 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| a2*c1 + a3*(c1+c2) + a4*(c1+c2+c3) - 0.5 = |       5.9605e-08 |          5.9605e-08 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| a2*d1 + a3*(d1+d2) + a4*(d1+d2+d3) - 0.5 = |       5.4836e-06 |          5.4836e-06 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| b2*c1 + b3*(c1+c2) + b4*(c1+c2+c3) - 0.5 = |       5.0366e-06 |          5.0366e-06 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| b2*d1 + b3*(d1+d2) + b4*(d1+d2+d3) - 0.5 = |       5.9605e-07 |          5.9605e-07 |\n",
      "+--------------------------------------------+------------------+---------------------+\n",
      "| c2*d1 + c3*(d1+d2) + c4*(d1+d2+d3) - 0.5 = |       5.3346e-06 |          5.3346e-06 |\n",
      "+--------------------------------------------+------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "func_res_model, solution  = multilayer_perceptron(weights, biases)\n",
    "solution = list(solution.numpy()[0])\n",
    "\n",
    "print('SOLUTION')\n",
    "for i, var in enumerate(variables):\n",
    "    print(f'{var} = {solution[i]:.16f}')\n",
    "    \n",
    "print()\n",
    "print('RESIDUALS')\n",
    "headers = ['Equation', 'Model residual', 'Function residual']\n",
    "table_data = []\n",
    "\n",
    "for f, func in enumerate(equations):\n",
    "    eq = equations_sym[f]\n",
    "    equation = lambdify(variables_sym, eq)\n",
    "    func_res_eq = abs(equation(*solution))\n",
    "    table_data.append([f'{func} = ', f'{abs(func_res_model[0, f]):.4e}', f'{func_res_eq:.4e}'])\n",
    "\n",
    "print(tabulate(table_data, headers=headers, tablefmt='grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nn-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
