{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scirp.org/journal/paperinformation.aspx?paperid=67010\n",
    "\n",
    "Example 1\n",
    "\n",
    "$F_1(x_1,x_2) = e^{x_1} + x_1 x_2 - 1 = 0$\n",
    "\n",
    "$F_2(x_1,x_2) = \\sin(x_1 x_2) + x_1 + x_2 - 1 = 0$\n",
    "\n",
    "Solution: $(x_1, x_2) = (0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 17:48:20.718034: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-30 17:48:20.719836: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-30 17:48:20.755169: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-30 17:48:20.755198: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-30 17:48:20.755219: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-30 17:48:20.761027: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-30 17:48:20.761437: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 17:48:21.672045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 2023\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1 (x1, x2):\n",
    "\n",
    "    return math.exp(x1) + x1 * x2 - 1\n",
    "\n",
    "def F2 (x1, x2):\n",
    "\n",
    "    return math.sin(x1 * x2)  + x1 + x2 - 1\n",
    "\n",
    "\n",
    "# define activation function for layer2\n",
    "def activation_layer2(layer):\n",
    "\n",
    "    x1_val = layer[0, 0]\n",
    "    x2_val = layer[0, 1]\n",
    "    exp_func = tf.exp(x1_val)\n",
    "    x1x2 = tf.multiply(x1_val, x2_val)\n",
    "    sin_func = tf.sin(x1x2)\n",
    "\n",
    "\n",
    "    layer_act = tf.Variable([[\n",
    "        exp_func, \n",
    "        x1x2,\n",
    "        sin_func,\n",
    "        x1_val,\n",
    "        x2_val,\n",
    "    ]], dtype=tf.float32)\n",
    "\n",
    "    layer_act = tf.reshape(layer_act, [1, 5])  # Reshape to [1, 5]\n",
    "    \n",
    "    return layer_act \n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "\n",
    "    # Reshape input if necessary, matching the shape of the first layer's weights\n",
    "    x = tf.reshape(x, [1, -1])  # Adjust the shape as needed\n",
    "\n",
    "    print(f'x.shape: {x.shape}')\n",
    "    print(f'weights[w12].shape: {weights[\"w12\"].shape}')\n",
    "    print(tf.matmul(x, weights['w12']).shape)\n",
    "    print(f'biases[b12].shape: {biases[\"b12\"].shape}')\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w12']), biases['b12'])\n",
    "\n",
    "    print(f'layer_1.shape: {layer_1.shape}')\n",
    "\n",
    "    layer_2 = activation_layer2(layer_1)\n",
    "\n",
    "    # Output fully connected layer\n",
    "    output = tf.add(tf.matmul(layer_2, weights['w34']), biases['out'])\n",
    "\n",
    "    return output, layer_1\n",
    "\n",
    "\n",
    "# Train step\n",
    "def train_step():\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        output, _ = multilayer_perceptron(tf.constant(1.0, dtype=tf.float32), weights, biases)\n",
    "\n",
    "        print(output)\n",
    "\n",
    "        loss = tf.reduce_sum(tf.square(output))\n",
    "\n",
    "    print(loss, weights['w12'])\n",
    "\n",
    "    trainable_variables = [weights['w12']]  # list containing only 'w12'\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    print(gradients)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 51  #   5000 + 1\n",
    "display_step = training_steps // 10\n",
    "\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "num_input = 1 # input layer\n",
    "num_hidden = [2, 5]\n",
    "num_output = 2 # output layer\n",
    "\n",
    "\n",
    "# Given by whether the functions in F1 and F2 contains the variables x1 and x2\n",
    "w23_flags = [[True, True, True, True, False], [False, True, True, False, True]]  \n",
    "w23_flags = tf.constant(w23_flags, dtype=tf.bool)\n",
    "\n",
    "# Initialize the weights (w23) with zeros\n",
    "w23 = tf.constant(tf.zeros([2, 5], dtype=tf.float32))\n",
    "# Set the weights to 1 where func_flags is True\n",
    "w23 = tf.where(w23_flags, tf.ones_like(w23), w23)\n",
    "w23 = tf.transpose(w23)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # Variables x1 and x2\n",
    "    'w12': tf.Variable(tf.random.normal([num_input, num_hidden[0]])),\n",
    "    # Whether the functions in F1 and F2 contain the variables x1 and x2\n",
    "    'w23': w23,\n",
    "    # The coefficients of the functions in F1 and F2\n",
    "    # 'w34': tf.constant([[1, 1, 0, 0, 0], [0, 0, 1, 1, 1]], dtype=tf.float32),\n",
    "    'w34': tf.constant([[1, 0], [1, 0], [0, 1], [0, 1], [0, 1]], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b12': tf.constant([0, 0], dtype=tf.float32),\n",
    "    'b23': tf.constant(tf.zeros([2, 5], dtype=tf.float32)),\n",
    "    'out': tf.constant([[-1, -1]], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = Adam(learning_rate=learning_rate, name='custom_optimizer_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7000161, -1.6504266]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_aux = tf.Variable(tf.random.normal([num_input, num_hidden[0]])).numpy()\n",
    "x_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w23_aux = w23.numpy()\n",
    "w23_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7000161, -0.       ],\n",
       "       [ 1.7000161, -1.6504266],\n",
       "       [ 1.7000161, -1.6504266],\n",
       "       [ 1.7000161, -0.       ],\n",
       "       [ 0.       , -1.6504266]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_aux * w23_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (1, 1)\n",
      "weights[w12].shape: (1, 2)\n",
      "(1, 2)\n",
      "biases[b12].shape: (2,)\n",
      "layer_1.shape: (1, 2)\n",
      "tf.Tensor([[2.7787828 2.8605108]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(15.904156, shape=(), dtype=float32) <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[0.7724748, 2.0889547]], dtype=float32)>\n",
      "[None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[0.7724748, 2.0889547]], dtype=float32)>),).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(training_steps):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     current_loss \u001b[39m=\u001b[39m train_step()\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m display_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m => loss: \u001b[39m\u001b[39m{\u001b[39;00mcurrent_loss\u001b[39m:\u001b[39;00m\u001b[39m.10e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, trainable_variables)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mprint\u001b[39m(gradients)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(gradients, trainable_variables))\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/nn_system_ex1.ipynb#X20sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/venv-nn-systems/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1222\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1218\u001b[0m experimental_aggregate_gradients \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\n\u001b[1;32m   1219\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mexperimental_aggregate_gradients\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m )\n\u001b[1;32m   1221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m-> 1222\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate_gradients(grads_and_vars)\n\u001b[1;32m   1223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/venv-nn-systems/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1184\u001b[0m, in \u001b[0;36mOptimizer.aggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[39mreturn\u001b[39;00m grads_and_vars\n\u001b[1;32m   1183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[39mreturn\u001b[39;00m optimizer_utils\u001b[39m.\u001b[39;49mall_reduce_sum_gradients(grads_and_vars)\n",
      "File \u001b[0;32m/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/venv-nn-systems/lib/python3.10/site-packages/keras/src/optimizers/utils.py:33\u001b[0m, in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all-reduced gradients aggregated via summation.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m  List of (gradient, variable) pairs where gradients have been all-reduced.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(grads_and_vars)\n\u001b[0;32m---> 33\u001b[0m filtered_grads_and_vars \u001b[39m=\u001b[39m filter_empty_gradients(grads_and_vars)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m filtered_grads_and_vars:\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mstrategy_supports_no_merge_call():\n",
      "File \u001b[0;32m/media/jesus/GSUS-DATA/CANADA_docs/RaymondSpiteri_USASK_SK/RKTK_equation_learning/nn-non-linear-systems/venv-nn-systems/lib/python3.10/site-packages/keras/src/optimizers/utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[1;32m     76\u001b[0m     variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     82\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     83\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloss. If you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using `model.compile()`, did you forget to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide a `loss` argument?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]),\n\u001b[1;32m     87\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[0.7724748, 2.0889547]], dtype=float32)>),)."
     ]
    }
   ],
   "source": [
    "for i in range(training_steps):\n",
    "       \n",
    "    current_loss = train_step()\n",
    "    if i % display_step == 0:\n",
    "        print(f\"epoch {i} => loss: {current_loss:.10e} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w12': <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.6096401 , -0.09947427]], dtype=float32)>,\n",
       " 'w23': <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 0., 1.]], dtype=float32)>,\n",
       " 'w34': <tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       " array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], dtype=float32)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.6096401>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-0.09947427>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.7791257>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.55044055>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_val, x2_val = weights['w12'][0][0], weights['w12'][0][1]\n",
    "\n",
    "display(\n",
    "    (x1_val, x2_val),\n",
    "    F1(x1_val, x2_val),\n",
    "    F2(x1_val, x2_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_1.shape: (1, 2)\n",
      "weights[w23].shape: (2, 5)\n",
      "(1, 5)\n",
      "biases[b23].shape: (2, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.11980605, 0.02033174],\n",
       "        [0.11980605, 0.02033174]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.6096401 , -0.09947427]], dtype=float32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_output = multilayer_perceptron(tf.constant(1.0, dtype=tf.float32), weights, biases)\n",
    "nn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.11980605>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.02033174>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.014766869>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = nn_output[0][0][0]\n",
    "out2 = nn_output[0][0][1]\n",
    "\n",
    "display(\n",
    "    out1,\n",
    "    out2,\n",
    ")\n",
    "\n",
    "tf.add(tf.square(out1), tf.square(out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6492906061048178, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(0.50056285, -0.00071701), F2(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nn-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
