{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scirp.org/journal/paperinformation.aspx?paperid=67010\n",
    "\n",
    "Example 1\n",
    "\n",
    "$F_1(x_1,x_2) = e^{x_1} + x_1 x_2 - 1 = 0$\n",
    "\n",
    "$F_2(x_1,x_2) = \\sin(x_1 x_2) + x_1 + x_2 - 1 = 0$\n",
    "\n",
    "Solution: $(x_1, x_2) = (0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 17:56:02.241040: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-06 17:56:02.242912: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-06 17:56:02.278049: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-06 17:56:02.278099: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-06 17:56:02.278130: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-06 17:56:02.284179: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-06 17:56:02.284849: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-06 17:56:03.270399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed_value = 2023\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1 (x1, x2):\n",
    "\n",
    "    return math.exp(x1) + x1 * x2 - 1\n",
    "\n",
    "def F2 (x1, x2):\n",
    "\n",
    "    return math.sin(x1 * x2)  + x1 + x2 - 1\n",
    "\n",
    "\n",
    "# Create a custom layer using Keras layers\n",
    "class Layer2(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Layer2, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1_val = inputs[0, 0]\n",
    "        x2_val = inputs[0, 1]\n",
    "        exp_func = tf.exp(x1_val)\n",
    "        x1x2 = tf.multiply(x1_val, x2_val)\n",
    "        sin_func = tf.sin(x1x2)\n",
    "\n",
    "        layer_act = tf.constant([\n",
    "            [\n",
    "                exp_func,\n",
    "                x1x2,\n",
    "                0.0,  # Change this to 0.0 to create a 2D tensor\n",
    "                sin_func,\n",
    "                x1_val,\n",
    "                x2_val,\n",
    "            ]\n",
    "        ], dtype=tf.float32)\n",
    "\n",
    "        layer_act = tf.reshape(layer_act, [1, 6])\n",
    "        return layer_act\n",
    "\n",
    "\n",
    "# define activation function for layer2\n",
    "def activation_layer2_ex1(layer):\n",
    "    x1_val = tf.squeeze(layer[0, 0])\n",
    "    x2_val = tf.squeeze(layer[0, 1])\n",
    "    exp_func = tf.exp(x1_val)\n",
    "    x1x2 = tf.multiply(x1_val, x2_val)\n",
    "    sin_func = tf.sin(x1x2)\n",
    "\n",
    "    layer_act = [\n",
    "        exp_func,\n",
    "        x1x2,\n",
    "        0.0,  # Change this to 0.0 to create a 2D tensor\n",
    "        sin_func,\n",
    "        x1_val,\n",
    "        x2_val,\n",
    "    ]\n",
    "\n",
    "    # Convert layer_act to a TensorFlow tensor\n",
    "    layer_2 = tf.convert_to_tensor(layer_act, dtype=tf.float32)\n",
    "    layer_2 = tf.reshape(layer_2, [1, 6])\n",
    "\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "def activation_layer2_ex3(layer):\n",
    "    \n",
    "    x1_val = tf.squeeze(layer[0, 0])\n",
    "    x2_val = tf.squeeze(layer[0, 1])\n",
    "    exp_func = tf.exp(x2_val)\n",
    "    x1_3 = tf.pow(x1_val, 3)\n",
    "\n",
    "    layer_act = [\n",
    "        x1_val,\n",
    "        x1_3,\n",
    "        x2_val,  # Change this to 0.0 to create a 2D tensor\n",
    "        x1_val,\n",
    "        x2_val,\n",
    "        exp_func,\n",
    "    ]\n",
    "\n",
    "    # Convert layer_act to a TensorFlow tensor\n",
    "    layer_2 = tf.convert_to_tensor(layer_act, dtype=tf.float32)\n",
    "    layer_2 = tf.reshape(layer_2, [1, 6])\n",
    "\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "\n",
    "    # Reshape input if necessary, matching the shape of the first layer's weights\n",
    "    x = tf.reshape(x, [1, -1])  # Adjust the shape as needed\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w12']), biases['b12'])\n",
    "\n",
    "    # layer_2 = tf.add(tf.matmul(layer_1, tf.transpose(weights['w23'])), biases['b23'])\n",
    "    layer_2 = activation_layer2_ex1(layer_1)\n",
    "    # layer_2 = activation_layer2_ex3(layer_1)\n",
    "    \n",
    "    # Output fully connected layer\n",
    "    output = tf.add(tf.matmul(layer_2, weights['w34']), biases['out'])\n",
    "    \n",
    "    return output, layer_1\n",
    "\n",
    "\n",
    "def loss_function(weights, biases):\n",
    "    \n",
    "    output, _= multilayer_perceptron(tf.constant(1.0, dtype=tf.float32), weights, biases)\n",
    "\n",
    "    return tf.reduce_mean(tf.square(output))\n",
    "\n",
    "\n",
    "# Train step\n",
    "def train_step(weights, biases, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss = loss_function(weights, biases)\n",
    "\n",
    "    trainable_variables = [weights['w12']]  # list containing only 'w12'\n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 5001  #   5000 + 1\n",
    "display_step = training_steps // 10\n",
    "\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "num_input = 1 # input layer\n",
    "num_hidden = [2, 6]\n",
    "num_output = 2 # output layer\n",
    "\n",
    "\n",
    "# Given by whether the functions in F1 and F2 contains the variables x1 and x2\n",
    "w23_flags = [[True, True,  False, True, True, False], [False, True, False, True, False, True]]  \n",
    "w23_flags = tf.constant(w23_flags, dtype=tf.bool)\n",
    "\n",
    "# Initialize the weights (w23) with zeros\n",
    "w23 = tf.constant(tf.zeros(num_hidden, dtype=tf.float32))\n",
    "# Set the weights to 1 where func_flags is True\n",
    "w23 = tf.where(w23_flags, tf.ones_like(w23), w23)\n",
    "w23 = tf.transpose(w23)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # Variables x1 and x2\n",
    "    'w12': tf.Variable(tf.random.normal([num_input, num_hidden[0]])),\n",
    "    # Whether the functions in F1 and F2 contain the variables x1 and x2\n",
    "    'w23': w23,\n",
    "    # The coefficients of the functions in F1 and F2\n",
    "    'w34': tf.constant([[1, 0], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1]], dtype=tf.float32),\n",
    "    # 'w34': tf.constant([[3, 0], [1, 0], [1, 0], [0, 1], [0, 2], [0, 1]], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b12': tf.constant([0, 0], dtype=tf.float32),\n",
    "    'b23': tf.constant(tf.zeros([num_hidden[1]], dtype=tf.float32)),\n",
    "    'out': tf.constant([[-1, -1]], dtype=tf.float32),\n",
    "    # 'out': tf.constant([[1, -2]], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = Adam(learning_rate=learning_rate, name='custom_optimizer_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.29742685, -0.41895387]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['w12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18302704, -0.09167181]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_aux = tf.Variable(tf.random.normal([num_input, num_hidden[0]])).numpy()\n",
    "x_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 => loss: 8.0061846971511841e-01 \n",
      "epoch 500 => loss: 7.4189774750266224e-06 \n",
      "epoch 1000 => loss: 2.2737367544323206e-13 \n",
      "epoch 1500 => loss: 1.7763568394002505e-13 \n",
      "epoch 2000 => loss: 1.5809575870662229e-13 \n",
      "epoch 2500 => loss: 5.6843418860808015e-14 \n",
      "epoch 3000 => loss: 3.0198066269804258e-14 \n",
      "epoch 3500 => loss: 2.8421709430404007e-14 \n",
      "epoch 4000 => loss: 1.7763568394002505e-15 \n",
      "epoch 4500 => loss: 3.0198066269804258e-14 \n",
      "epoch 5000 => loss: 1.7763568394002505e-15 \n"
     ]
    }
   ],
   "source": [
    "for i in range(training_steps):\n",
    "       \n",
    "    current_loss = train_step(weights, biases, optimizer)\n",
    "    if i % display_step == 0:\n",
    "        print(f\"epoch {i} => loss: {current_loss:.16e} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-3.6665828e-08>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.99999994>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-1.1920929e-07>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-1.1920929e-07>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_val, x2_val = weights['w12'][0][0], weights['w12'][0][1]\n",
    "\n",
    "display(\n",
    "    (x1_val, x2_val),\n",
    "    F1(x1_val, x2_val),\n",
    "    F2(x1_val, x2_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 = -3.6665827707338394e-08, x2 = 0.9999999403953552\n",
      "Residuals: F1 = -1.1920928955078125e-07, F2 = -1.1920928955078125e-07\n"
     ]
    }
   ],
   "source": [
    "F1F2, solution  = multilayer_perceptron(tf.constant(1.0, dtype=tf.float32), weights, biases)\n",
    "\n",
    "print(f'x1 = {solution[0][0]}, x2 = {solution[0][1]}')\n",
    "print(f'Residuals: F1 = {F1F2[0][0]}, F2 = {F1F2[0][1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nn-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
